{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**The dataset contains various features related to voice measurements. Here's an overview of the first few rows:**\n",
        "\n",
        "- Each row appears to represent a voice recording, with multiple acoustic features measured for each recording.\n",
        "- The features include measurements like MDVP:Fo(Hz), MDVP:Fhi(Hz), MDVP:Flo(Hz), jitter, shimmer, and other voice parameters.\n",
        "- There's a column named status, which might indicate a particular condition or classification.\n",
        "- The name column seems to be an identifier for each recording."
      ],
      "metadata": {
        "id": "VlaYGFLM0yMb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWxt4HLm0wvQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/parkinsons.data'\n",
        "parkinsons_data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "parkinsons_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We will perform the following analyses:**\n",
        "\n",
        "- Descriptive Statistics: This will give us basic statistics for each feature, like mean, median, standard deviation, etc.\n",
        "- Data Quality Check: We'll look for missing values or anomalies in the dataset.\n",
        "Feature Distribution: Understanding the distribution of various features, possibly using histograms or box plots.\n",
        "- Correlation Analysis: Analyzing how different features are correlated with each other, particularly with the 'status' variable if it's a target variable.\n",
        "- Status Breakdown: If 'status' is a key variable (e.g., indicating a condition or classification), we can look at its distribution."
      ],
      "metadata": {
        "id": "7wUwqC9G1Cda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The comprehensive overview of the dataset, we can perform the following analyses:**\n",
        "\n",
        "1. **Descriptive Statistics**: This will give us basic statistics for each feature, like mean, median, standard deviation, etc.\n",
        "2. **Data Quality Check**: We'll look for missing values or anomalies in the dataset.\n",
        "3. **Feature Distribution**: Understanding the distribution of various features, possibly using histograms or box plots.\n",
        "4. **Correlation Analysis**: Analyzing how different features are correlated with each other, particularly with the 'status' variable if it's a target variable.\n",
        "5. **Status Breakdown**: If 'status' is a key variable (e.g., indicating a condition or classification), we can look at its distribution.\n",
        "\n",
        "Let's start with the first three points: Descriptive Statistics, Data Quality Check, and Feature Distribution. After that, we'll move on to Correlation Analysis and Status Breakdown. I'll begin with Descriptive Statistics and Data Quality Check.\n",
        "\n",
        "### Descriptive Statistics\n",
        "\n",
        "**The Summary of the descriptive statistics for the dataset:**\n",
        "\n",
        "- **Count**: The dataset has 195 entries.\n",
        "- **Mean and Standard Deviation**: These values provide an insight into the average and variability of each feature. For instance, the average frequency (`MDVP:Fo(Hz)`) is about 154 Hz with a standard deviation of 41 Hz, indicating some variability in voice frequency among the samples.\n",
        "- **Min and Max**: These values show the range of each feature. For example, the minimum and maximum values of `MDVP:Fhi(Hz)` are 102.145 Hz and 592.030 Hz, respectively.\n",
        "- **Quartiles (25%, 50%, 75%)**: These values give an idea about the distribution of the data. The 50% value (median) is particularly useful for understanding the central tendency of the data.\n",
        "\n",
        "### Data Quality Check\n",
        "\n",
        "- There are no missing values in any of the columns, which is excellent for analysis purposes.\n",
        "\n",
        "Given that the data is clean and well-structured, we can proceed to the next steps: Feature Distribution, Correlation Analysis, and Status Breakdown. Let's start by visualizing the distribution of some key features, including the target variable 'status'.\n",
        "\n",
        "**The distribution plots for selected features from the Parkinson's dataset are as follows:**\n",
        "\n",
        "1. **MDVP:Fo(Hz), MDVP:Fhi(Hz), MDVP:Flo(Hz)**: These features, representing various frequency measures of the voice, show somewhat skewed distributions. For example, `MDVP:Fo(Hz)` and `MDVP:Flo(Hz)` are right-skewed, indicating that lower frequencies are more common.\n",
        "\n",
        "2. **MDVP:Jitter(%)**: This measure of frequency variation is also right-skewed, suggesting that higher jitter percentages are less common.\n",
        "\n",
        "3. **MDVP:Shimmer**: Similar to Jitter, the shimmer values are right-skewed, indicating that most voice samples have lower shimmer.\n",
        "\n",
        "4. **HNR (Harmonics-to-Noise Ratio)**: This feature shows a more varied distribution, slightly skewed to the left.\n",
        "\n",
        "5. **Status**: It appears that the majority of the samples in the dataset are labeled with 'status' 1. The 'status' variable seems to be binary, possibly indicating the presence or absence of Parkinson's disease.\n",
        "\n",
        "Next, we'll perform a correlation analysis to understand how these features relate to each other and particularly to the 'status' variable. This can provide insights into which features might be important for predicting the 'status'.\n",
        "\n",
        "**The heatmap displays the correlation matrix for the dataset, providing insights into how different features are related to each other. Here are some key observations:**\n",
        "\n",
        "1. **Correlation with Status**: Several features show a significant correlation with the 'status' variable. For instance, features like `spread1`, `PPE` (Pitch Period Entropy), and `MDVP:Shimmer` have relatively strong positive correlations with 'status'. This suggests they could be important factors in distinguishing between the two statuses.\n",
        "\n",
        "2. **Highly Correlated Features**: There are groups of features that are highly correlated with each other. For example, `MDVP:Shimmer`, `Shimmer:DDA`, `MDVP:Shimmer(dB)`, `Shimmer:APQ3`, and `Shimmer:APQ5` are all strongly correlated. This is expected as they are different measures of voice amplitude variation (shimmer).\n",
        "\n",
        "3. **Negative Correlations**: Some features show a negative correlation with 'status', such as `HNR` (Harmonics-to-Noise Ratio). A higher HNR typically indicates a healthier voice signal, which aligns with it being negatively correlated with Parkinson's disease status.\n",
        "\n",
        "These correlations are useful for understanding relationships within the data and can guide further analysis, such as feature selection for predictive modeling."
      ],
      "metadata": {
        "id": "38UPOkXt1hrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive Statistics\n",
        "descriptive_stats = parkinsons_data.describe()\n",
        "\n",
        "# Data Quality Check\n",
        "missing_values = parkinsons_data.isnull().sum()\n",
        "\n",
        "descriptive_stats"
      ],
      "metadata": {
        "id": "6gDKW7R31eSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values"
      ],
      "metadata": {
        "id": "M2sqzZAR1-3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the aesthetic style of the plots\n",
        "sns.set()\n",
        "\n",
        "# Selecting a subset of columns for distribution plots (for simplicity)\n",
        "columns_to_plot = ['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', 'MDVP:Jitter(%)', 'MDVP:Shimmer', 'HNR', 'status']\n",
        "\n",
        "# Plotting distributions for the selected features\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, column in enumerate(columns_to_plot, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    if column == 'status':\n",
        "        sns.countplot(data=parkinsons_data, x=column)\n",
        "    else:\n",
        "        sns.histplot(parkinsons_data[column], kde=True)\n",
        "    plt.title(column)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "V6Ol868b2JKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation analysis\n",
        "correlation_matrix = parkinsons_data.corr()\n",
        "\n",
        "# Plotting the correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ChNIupQ92MPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Predictive Modeling**: Developing a model to predict the 'status' (likely indicating the presence or absence of Parkinson's disease) based on the other features.\n",
        "2. **Principal Component Analysis (PCA)**: Reducing the dimensionality of the data to understand the most important features.\n",
        "3. **Outlier Detection**: Identifying any outliers in the dataset that might affect analyses and models.\n",
        "4. **Advanced Feature Analysis**: Examining the relationships and importance of specific features in more detail.\n",
        "\n",
        "Let's begin with Predictive Modeling, where we'll build a model to predict the 'status' based on other features in the dataset. We'll use a simple classification model for this purpose.\n",
        "\n",
        "First, we'll prepare the data by splitting it into features (X) and the target variable (y), and then divide it into training and testing sets. After that, we'll train a classifier and evaluate its performance. Let's start with the data preparation and model training."
      ],
      "metadata": {
        "id": "rGtbACcK2SnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Preparing the data\n",
        "X = parkinsons_data.drop(['name', 'status'], axis=1)  # Features\n",
        "y = parkinsons_data['status']  # Target variable\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Creating and training a RandomForest Classifier\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluating the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "accuracy\n"
      ],
      "metadata": {
        "id": "ezowShzQ2QlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_rep)"
      ],
      "metadata": {
        "id": "64Z7qnph2ic2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictive Modeling Results\n",
        "\n",
        "The RandomForest Classifier was used to predict the 'status' variable. Here are the results:\n",
        "\n",
        "- **Accuracy**: The model achieved an accuracy of approximately 93.2%. This indicates a high level of performance in classifying the status correctly.\n",
        "- **Classification Report**:\n",
        "    - **Precision**: Measures the accuracy of positive predictions. For status 0 (likely representing absence of Parkinson's), the precision is 92%, and for status 1 (likely presence of Parkinson's), it's 93%.\n",
        "    - **Recall**: Indicates the ability to find all positive instances. For status 0, the recall is 80%, and for status 1, it's 98%.\n",
        "    - **F1-score**: A weighted average of precision and recall. The F1-scores are 86% for status 0 and 96% for status 1.\n",
        "\n",
        "The high values in these metrics suggest that the model is quite effective at distinguishing between the two statuses. The better performance on status 1 might be due to a larger number of samples for this class in the dataset, as seen in the earlier distribution plot.\n"
      ],
      "metadata": {
        "id": "IBwE_NUF2eUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Principal Component Analysis (PCA)**: This will help us understand the underlying structure of the data and reduce the number of features while retaining most of the variance in the data.\n",
        "2. **Outlier Detection**: We'll identify outliers in the dataset, which can impact the performance of predictive models.\n",
        "3. **Advanced Feature Analysis**: This involves examining specific features in more detail, exploring their relationships and impact on the target variable.\n",
        "\n",
        "Let's start with Principal Component Analysis (PCA) to reduce the dimensionality of the dataset and identify the most significant components.\n",
        "\n",
        "### Principal Component Analysis (PCA) Results\n",
        "\n",
        "- **Number of Components**: The PCA reduced the dataset to 8 principal components while retaining approximately 95.77% of the variance.\n",
        "- **Explained Variance**: This high level of explained variance suggests that these 8 components capture most of the information present in the original dataset.\n",
        "\n",
        "By reducing the dimensionality to 8 components from the original number of features, we can simplify the dataset while still retaining most of the information. This can be particularly useful for visualization, further analysis, or more efficient modeling.\n",
        "\n",
        "Next, let's move to Outlier Detection. We'll use a statistical method to identify outliers in the dataset. Outliers can significantly affect the results of data analysis and statistical modeling. Let's proceed with that.\n",
        "\n",
        "### Outlier Detection Results\n",
        "\n",
        "- **Number of Outliers Detected**: 14 unique data points in the dataset have been identified as outliers based on the Z-score method. These data points have values in one or more features that are significantly different from the average (typically, a Z-score greater than 3 is considered an outlier).\n",
        "- **Outlier Details**: The outliers are distributed across various features. For example, the data point at index 73 is an outlier in the feature corresponding to index 1.\n",
        "\n",
        "Outliers can arise due to various reasons such as measurement errors, data entry errors, or genuine variance in the data. Depending on the context, you might choose to investigate these outliers further, or exclude them from certain analyses to avoid skewing the results.\n",
        "\n",
        "Finally, we can move on to Advanced Feature Analysis. This step involves a deeper look into specific features to understand their individual impact and relationships with the target variable ('status'). This can include analyses like feature importance from the model, bivariate analyses, or looking into interaction effects between features.\n"
      ],
      "metadata": {
        "id": "NJu7xORS2trz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Standardizing the features before applying PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Applying PCA\n",
        "pca = PCA(n_components=0.95)  # retain 95% of the variance\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Number of components chosen and the explained variance ratio\n",
        "num_components = pca.n_components_\n",
        "explained_variance = np.sum(pca.explained_variance_ratio_)\n",
        "\n",
        "num_components, explained_variance\n"
      ],
      "metadata": {
        "id": "OkF-i8V222Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Detecting outliers using Z-score\n",
        "# Note: Typically, data points with a Z-score greater than 3 are considered outliers\n",
        "z_scores = np.abs(stats.zscore(X_scaled))\n",
        "outliers = np.where(z_scores > 3)\n",
        "\n",
        "# Number of outliers detected\n",
        "num_outliers = len(set(outliers[0]))\n",
        "num_outliers\n"
      ],
      "metadata": {
        "id": "6nGjeJSZ2tdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers"
      ],
      "metadata": {
        "id": "yXcvIxYM2eur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "0GYjCcvj3MLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Perform Logistic Regression**: We will fit a logistic regression model to the dataset.\n",
        "2. **Model Evaluation**: Assess the performance of the model.\n",
        "3. **Visualization**:\n",
        "    - **Coefficient Plot**: To visualize the impact of each feature.\n",
        "    - **Confusion Matrix**: For a visual representation of the model's performance.\n",
        "    - **ROC Curve**: To assess the model's ability to discriminate between the two classes.\n",
        "### Logistic Regression Analysis Results\n",
        "\n",
        "1. **Model Accuracy**: The logistic regression model achieved an accuracy of approximately 84.75% on the test set.\n",
        "2. **Confusion Matrix**:\n",
        "   - True Negative (TN): 9\n",
        "   - False Positive (FP): 6\n",
        "   - False Negative (FN): 3\n",
        "   - True Positive (TP): 41\n",
        "3. **ROC AUC Score**: The area under the ROC curve is approximately 0.898, indicating a good discriminatory ability of the model between the two classes.\n",
        "\n",
        "### Visualizations\n",
        "\n",
        "1. **Coefficient Plot**: Showing the impact of each feature on the model.\n",
        "2. **Confusion Matrix**: A visual representation of the model's performance.\n",
        "3. **ROC Curve**: To visualize the model's discriminative ability.\n",
        "\n",
        "#### 1. Coefficient Plot\n",
        "\n",
        "- **Positive Coefficients**: Increase the log-odds of the target variable being 1 (indicating a higher likelihood of Parkinson's presence, if that's what 'status' 1 represents).\n",
        "- **Negative Coefficients**: Decrease the log-odds of the target variable being 1.\n",
        "\n",
        "#### 2. Confusion Matrix\n",
        "\n",
        "- **True Positives (TP)**: 41 (correctly predicted status 1)\n",
        "- **True Negatives (TN)**: 9 (correctly predicted status 0)\n",
        "- **False Positives (FP)**: 6 (incorrectly predicted as status 1)\n",
        "- **False Negatives (FN)**: 3 (incorrectly predicted as status 0)\n",
        "\n",
        "\n",
        "#### 3. ROC Curve\n",
        "\n",
        "The Receiver Operating Characteristic (ROC) Curve and the Area Under the Curve (AUC) provide insights into the model's classification ability:\n",
        "\n",
        "- The blue line represents the ROC curve for the logistic regression model, with an AUC of approximately 0.90.\n",
        "- The gray dashed line represents a purely random classifier (AUC = 0.50).\n",
        "- The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\n",
        "\n",
        "An AUC close to 1 indicates a high ability of the model to differentiate between the two classes (status 0 and status 1). In this case, the AUC of 0.90 suggests that the model has a good discriminative ability."
      ],
      "metadata": {
        "id": "KffXjLCH3P3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Creating and training a Logistic Regression model\n",
        "log_reg_model = LogisticRegression(max_iter=1000)\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred_log_reg = log_reg_model.predict(X_test)\n",
        "y_pred_proba_log_reg = log_reg_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Model Evaluation\n",
        "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_log_reg)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba_log_reg)\n",
        "\n",
        "# Coefficients for visualization\n",
        "coefficients = log_reg_model.coef_[0]\n",
        "\n",
        "accuracy_log_reg\n"
      ],
      "metadata": {
        "id": "mtT3SCPO27Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix"
      ],
      "metadata": {
        "id": "i3440Ta73dcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc"
      ],
      "metadata": {
        "id": "VASEIhGJ3fGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coefficients"
      ],
      "metadata": {
        "id": "6YSP890E3hEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coefficient Plot\n",
        "feature_names = X.columns\n",
        "coefficients = log_reg_model.coef_[0]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "coeff_plot = sns.barplot(x=coefficients, y=feature_names)\n",
        "plt.title('Logistic Regression Coefficients')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bkpC2GHt3iH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix Plot\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "K5q8RkK93l-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC Curve Plot\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_log_reg)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hdTceoid3pIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "RHaELNcV6BGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Creating and training a Gaussian Naive Bayes model\n",
        "naive_bayes_model = GaussianNB()\n",
        "naive_bayes_model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred_nb = naive_bayes_model.predict(X_test)\n",
        "y_pred_proba_nb = naive_bayes_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Model Evaluation\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)\n",
        "roc_auc_nb = roc_auc_score(y_test, y_pred_proba_nb)\n",
        "\n",
        "accuracy_nb, conf_matrix_nb, roc_auc_nb\n"
      ],
      "metadata": {
        "id": "RJD95CSl3sFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix Plot for Naive Bayes\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix_nb, annot=True, fmt=\"d\", cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Naive Bayes Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KpQnJWTl7NH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC Curve Plot for Naive Bayes\n",
        "fpr_nb, tpr_nb, thresholds_nb = roc_curve(y_test, y_pred_proba_nb)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(fpr_nb, tpr_nb, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc_nb)\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Naive Bayes ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aNOYizdA6eVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Fit a Naive Bayes Model**: Apply the Naive Bayes algorithm to the dataset.\n",
        "2. **Evaluate the Model**: Assess the model's performance.\n",
        "3. **Visualizations**:\n",
        "   - **Confusion Matrix**: To visualize the performance of the model.\n",
        "   - **ROC Curve**: Assessing the model's discriminative ability.\n",
        "\n",
        "\n",
        "### Naive Bayes Model Analysis Results\n",
        "\n",
        "1. **Model Accuracy**: The Naive Bayes model achieved an accuracy of approximately 74.58% on the test set.\n",
        "2. **Confusion Matrix**:\n",
        "   - True Negative (TN): 12\n",
        "   - False Positive (FP): 3\n",
        "   - False Negative (FN): 12\n",
        "   - True Positive (TP): 32\n",
        "3. **ROC AUC Score**: The area under the ROC curve is approximately 0.853, indicating a good ability of the model to differentiate between the two classes.\n",
        "\n",
        "### Visualizations\n",
        "\n",
        "\n",
        "1. **Confusion Matrix**: To understand the distribution of true and false predictions.\n",
        "2. **ROC Curve**: To assess the model's ability to discriminate between the classes.\n",
        "\n",
        "#### 1. Confusion Matrix Visualization\n",
        "- **True Positives (TP)**: 32 (correctly predicted status 1)\n",
        "- **True Negatives (TN)**: 12 (correctly predicted status 0)\n",
        "- **False Positives (FP)**: 3 (incorrectly predicted as status 1)\n",
        "- **False Negatives (FN)**: 12 (incorrectly predicted as status 0)\n",
        "\n",
        "This matrix provides a clear picture of the model's performance in terms of correct and incorrect predictions.\n",
        "\n",
        "#### 2. ROC Curve Visualization\n",
        "\n",
        "- The blue line represents the ROC curve for the Naive Bayes model, with an AUC of approximately 0.853.\n",
        "- The gray dashed line represents a random classifier (AUC = 0.50).\n",
        "- The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various thresholds.\n",
        "\n",
        "An AUC of 0.853 suggests that the Naive Bayes model has a good ability to distinguish between the two classes (status 0 and status 1). While not as high as the logistic regression model's performance, it still indicates a respectable level of discriminative ability.\n"
      ],
      "metadata": {
        "id": "M7gBR0DW7FbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Nearest Neighbors\n"
      ],
      "metadata": {
        "id": "sd-NqXFL7TRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Creating and training a KNN model (with k=5)\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "y_pred_proba_knn = knn_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Model Evaluation\n",
        "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
        "roc_auc_knn = roc_auc_score(y_test, y_pred_proba_knn)\n",
        "\n",
        "accuracy_knn, conf_matrix_knn, roc_auc_knn\n"
      ],
      "metadata": {
        "id": "ZUdD11eR6idj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix Plot for KNN\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix_knn, annot=True, fmt=\"d\", cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('KNN Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iD0b0wwo7WTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC Curve Plot for KNN\n",
        "fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, y_pred_proba_knn)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(fpr_knn, tpr_knn, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc_knn)\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('KNN ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bKMoKtXe7ZDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploring the impact of different 'k' values on KNN model accuracy\n",
        "k_values = range(1, 21)\n",
        "accuracy_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred_k = knn.predict(X_test)\n",
        "    accuracy_k = accuracy_score(y_test, y_pred_k)\n",
        "    accuracy_scores.append(accuracy_k)\n",
        "\n",
        "# Plotting accuracy vs. k values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, accuracy_scores, marker='o', linestyle='-', color='blue')\n",
        "plt.title('KNN Accuracy for Different k Values')\n",
        "plt.xlabel('k (Number of Neighbors)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(k_values)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BEbCAXk77cmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Fit a KNN Model**: Apply KNN to the dataset.\n",
        "2. **Evaluate the Model**: Assess the model's performance.\n",
        "3. **Visualizations**:\n",
        "   - **Confusion Matrix**: For visualizing the performance of the model.\n",
        "   - **ROC Curve**: To assess the discriminative ability of the model.\n",
        "   - **K-value Analysis**: Visualizing the impact of different 'k' values on model accuracy.\n",
        "\n",
        "### K-Nearest Neighbors (KNN) Model Analysis Results\n",
        "\n",
        "1. **Model Accuracy**: The KNN model (with k=5) achieved an accuracy of approximately 83.05% on the test set.\n",
        "2. **Confusion Matrix**:\n",
        "   - True Negative (TN): 8\n",
        "   - False Positive (FP): 7\n",
        "   - False Negative (FN): 3\n",
        "   - True Positive (TP): 41\n",
        "3. **ROC AUC Score**: The area under the ROC curve is approximately 0.769, indicating a reasonable ability of the model to differentiate between the two classes.\n",
        "\n",
        "### Visualizations\n",
        "\n",
        "\n",
        "1. **Confusion Matrix**: To visualize the model's classification accuracy.\n",
        "2. **ROC Curve**: Assessing the discriminative ability of the model.\n",
        "3. **K-value Analysis**: Exploring how different 'k' values impact the model accuracy.\n",
        "\n",
        "#### 1. Confusion Matrix Visualization\n",
        "\n",
        "- **True Positives (TP)**: 41 (correctly predicted status 1)\n",
        "- **True Negatives (TN)**: 8 (correctly predicted status 0)\n",
        "- **False Positives (FP)**: 7 (incorrectly predicted as status 1)\n",
        "- **False Negatives (FN)**: 3 (incorrectly predicted as status 0)\n",
        "\n",
        "#### 2. ROC Curve Visualization\n",
        "\n",
        "- The blue line represents the ROC curve for the KNN model, with an AUC of approximately 0.769.\n",
        "- The gray dashed line represents a random classifier (AUC = 0.50).\n",
        "- The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various thresholds.\n",
        "\n",
        "An AUC of 0.769 suggests a reasonable ability of the KNN model to differentiate between the two classes, though it's lower compared to the logistic regression and Naive Bayes models we previously examined.\n",
        "\n",
        "#### 3. K-value Analysis\n",
        "\n",
        "Finally, let's analyze how different 'k' values (number of neighbors) impact the accuracy of the KNN model. We'll plot the model accuracy as a function of 'k' to determine the best 'k' value for this dataset.\n",
        "\n",
        "The plot above illustrates the impact of different 'k' values (number of neighbors) on the accuracy of the K-Nearest Neighbors model:\n",
        "\n",
        "- The accuracy of the KNN model varies with different 'k' values.\n",
        "- There is a noticeable fluctuation in accuracy as 'k' changes, indicating the sensitivity of the model to the choice of 'k'.\n",
        "- The plot can be used to select an optimal 'k' value that maximizes accuracy. In this case, we observe that certain 'k' values achieve higher accuracy than others.\n",
        "\n",
        "Choosing the right 'k' value is crucial for KNN models, as it balances the bias-variance tradeoff. A very small 'k' can make the model sensitive to noise, while a very large 'k' might oversimplify the model.\n"
      ],
      "metadata": {
        "id": "UOaBp_7K7gu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Decision Tree Classifier"
      ],
      "metadata": {
        "id": "fuMzEeNM_2pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Creating and training a Decision Tree model\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred_dt = decision_tree_model.predict(X_test)\n",
        "\n",
        "# Model Evaluation\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "conf_matrix_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "\n",
        "accuracy_dt, conf_matrix_dt\n"
      ],
      "metadata": {
        "id": "8F2Iq4oz7fWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import plot_tree\n",
        "\n",
        "# Visualizing the Decision Tree\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(decision_tree_model, filled=True, feature_names=X.columns, class_names=['0', '1'], fontsize=10)\n",
        "plt.title(\"Decision Tree Visualization\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "e6awoI2Q_4jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix Plot for Decision Tree\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix_dt, annot=True, fmt=\"d\", cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Decision Tree Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WdkjLZ6C_6rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance from Decision Tree\n",
        "feature_importances_dt = decision_tree_model.feature_importances_\n",
        "\n",
        "# Plotting feature importances\n",
        "plt.figure(figsize=(12, 8))\n",
        "importance_plot = sns.barplot(x=feature_importances_dt, y=X.columns)\n",
        "plt.title('Feature Importances in Decision Tree')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KIzhdYyJ_9xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Fitting a Decision Tree Model**: We will train a Decision Tree classifier on the dataset.\n",
        "2. **Evaluating the Model**: Assessing the performance of the model.\n",
        "3. **Visualizations**:\n",
        "   - **Tree Visualization**: To visually inspect the structure of the trained decision tree.\n",
        "   - **Confusion Matrix**: For a clear representation of the model's prediction accuracy.\n",
        "   - **Feature Importance**: To understand which features are most influential in making predictions.\n",
        "\n",
        "\n",
        "### Decision Tree Model Analysis Results\n",
        "\n",
        "1. **Model Accuracy**: The Decision Tree model achieved an accuracy of approximately 86.44% on the test set.\n",
        "2. **Confusion Matrix**:\n",
        "   - True Negative (TN): 12\n",
        "   - False Positive (FP): 3\n",
        "   - False Negative (FN): 5\n",
        "   - True Positive (TP): 39\n",
        "\n",
        "### Visualizations\n",
        "\n",
        "\n",
        "1. **Tree Visualization**: To inspect the structure of the decision tree.\n",
        "2. **Confusion Matrix**: For a clear representation of the model's prediction accuracy.\n",
        "3. **Feature Importance Plot**: To understand which features are most influential.\n",
        "\n",
        "#### 1. Tree Visualization\n",
        "\n",
        "The visualization above shows the structure of the trained Decision Tree. Each node in the tree represents a decision based on the value of a certain feature, leading to a classification at the leaf nodes. The colors represent the classes, with shades indicating the majority class in each node.\n",
        "\n",
        "This tree structure is useful for understanding the decision-making process of the model and how different features contribute to these decisions.\n",
        "\n",
        "#### 2. Confusion Matrix Visualization\n",
        "\n",
        "- **True Positives (TP)**: 39 (correctly predicted status 1)\n",
        "- **True Negatives (TN)**: 12 (correctly predicted status 0)\n",
        "- **False Positives (FP)**: 3 (incorrectly predicted as status 1)\n",
        "- **False Negatives (FN)**: 5 (incorrectly predicted as status 0)\n",
        "\n",
        "This matrix helps in understanding the balance between correctly and incorrectly classified instances by the Decision Tree model.\n",
        "\n",
        "#### 3. Feature Importance Plot\n",
        "\n",
        "The bar plot above illustrates the feature importances as identified by the Decision Tree model. Each bar represents a feature's importance in making predictions, with higher bars indicating more important features.\n",
        "\n",
        "From this visualization, you can discern which features the Decision Tree model found most useful in classifying the data. This insight can be valuable for feature selection in future modeling or for understanding the underlying patterns in the data."
      ],
      "metadata": {
        "id": "OeEUbYPuABqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machines"
      ],
      "metadata": {
        "id": "wNm_3bNECu61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Creating and training an SVM model with RBF kernel\n",
        "svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "y_pred_proba_svm = svm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Model Evaluation\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
        "roc_auc_svm = roc_auc_score(y_test, y_pred_proba_svm)\n",
        "\n",
        "accuracy_svm, conf_matrix_svm, roc_auc_svm\n"
      ],
      "metadata": {
        "id": "gKYQwlFT__qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC Curve Plot for SVM\n",
        "fpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test, y_pred_proba_svm)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(fpr_svm, tpr_svm, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc_svm)\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('SVM ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oaAFPV4SCAvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Fitting an SVM Model**: We will train an SVM classifier on the dataset.\n",
        "2. **Evaluating the Model**: Assessing the performance of the model.\n",
        "3. **Visualizations**:\n",
        "   - **Confusion Matrix**: For a clear representation of the model's prediction accuracy.\n",
        "   - **ROC Curve**: To assess the discriminative ability of the model.\n",
        "   - **Decision Boundary (if feasible)**: To visualize how the SVM separates the classes (note that this is more straightforward in lower-dimensional spaces).\n",
        "\n",
        "\n",
        "### Support Vector Machine (SVM) Model Analysis Results\n",
        "\n",
        "1. **Model Accuracy**: The SVM model achieved an accuracy of approximately 81.36% on the test set.\n",
        "2. **Confusion Matrix**:\n",
        "   - True Negative (TN): 4\n",
        "   - False Positive (FP): 11\n",
        "   - False Negative (FN): 0\n",
        "   - True Positive (TP): 44\n",
        "3. **ROC AUC Score**: The area under the ROC curve is approximately 0.730, indicating a moderate ability of the model to differentiate between the two classes.\n",
        "\n",
        "### Visualizations\n",
        "\n",
        "\n",
        "1. **Confusion Matrix**: To visualize the model's classification accuracy.\n",
        "2. **ROC Curve**: To assess the discriminative ability of the model.\n",
        "\n",
        "#### 1. Confusion Matrix Visualization\n",
        "\n",
        "- **True Positives (TP)**: 44 (correctly predicted status 1)\n",
        "- **True Negatives (TN)**: 4 (correctly predicted status 0)\n",
        "- **False Positives (FP)**: 11 (incorrectly predicted as status 1)\n",
        "- **False Negatives (FN)**: 0 (no incorrect predictions as status 0)\n",
        "\n",
        "\n",
        "#### 2. ROC Curve Visualization\n",
        "\n",
        "The ROC Curve for the SVM model is presented above:\n",
        "\n",
        "- The blue line represents the ROC curve for the SVM model, with an AUC of approximately 0.730.\n",
        "- The gray dashed line represents a random classifier (AUC = 0.50).\n",
        "- The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various thresholds.\n",
        "\n",
        "An AUC of 0.730 suggests a moderate ability of the SVM model to differentiate between the two classes. While this is a bit lower compared to some of the other models we examined, it still indicates a reasonable level of discriminative ability.\n",
        "\n",
        "#### Decision Boundary Visualization\n",
        "\n",
        "Visualizing the decision boundary of an SVM in a high-dimensional space (like our dataset) is complex. Typically, decision boundary visualizations are more insightful for datasets with two or three features. However, we can perform a dimensionality reduction (e.g., using PCA) to project the data onto a lower-dimensional space and visualize the decision boundary there. This will be a simplified representation and may not capture all the nuances of the high-dimensional decision process."
      ],
      "metadata": {
        "id": "jrSHmxMuCNVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance metrics for all models\n",
        "model_performance = {\n",
        "    'Model': ['Logistic Regression', 'Naive Bayes', 'KNN', 'Decision Tree', 'SVM'],\n",
        "    'Accuracy': [accuracy_log_reg, accuracy_nb, accuracy_knn, accuracy_dt, accuracy_svm],\n",
        "    'ROC AUC': [roc_auc, roc_auc_nb, roc_auc_knn, 'N/A', roc_auc_svm]\n",
        "}\n",
        "\n",
        "# Converting to DataFrame for easier plotting\n",
        "model_performance_df = pd.DataFrame(model_performance)\n",
        "\n",
        "# Plotting model performance\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plotting Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(x='Accuracy', y='Model', data=model_performance_df, palette='viridis')\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.xlabel('Accuracy')\n",
        "plt.ylabel('Model')\n",
        "\n",
        "# Plotting ROC AUC\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(x='ROC AUC', y='Model', data=model_performance_df[model_performance_df['ROC AUC'] != 'N/A'], palette='magma')\n",
        "plt.title('Model ROC AUC Comparison')\n",
        "plt.xlabel('ROC AUC')\n",
        "plt.ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BucAUujICEpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting a subset of features for individual analysis\n",
        "features_for_individual_analysis = ['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', 'HNR', 'spread1']\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Creating boxplots for each selected feature against the 'status'\n",
        "for i, feature in enumerate(features_for_individual_analysis, 1):\n",
        "    plt.subplot(3, 2, i)\n",
        "    sns.boxplot(x='status', y=feature, data=parkinsons_data)\n",
        "    plt.title(f'Boxplot of {feature} by Status')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Zu5ctCpbFwKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QxTQUhGCGFb7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}